{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "\n",
    "from nibabel.testing import data_path\n",
    "import nibabel as nib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Global variables\n",
    "ROOT_DIR = \"/net/i/broseau/Travail/3A/FouilleExtrcVisu/2019_ENSEIRB_Skull/\"\n",
    "DATA_DIR = '/tmp/broseau_data/nifti/'\n",
    "TRAIN_DATA_PROPORTION = 0.8 # Proportion of data used for training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initializing dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added to train data: 0 :\t /tmp/broseau_data/nifti/527/Shank_527.ibw.nii\n",
      "Added to label data: 0 :\t /tmp/broseau_data/nifti/527/Shank_527-labels.nii\n",
      "Added to label data: 1 :\t /tmp/broseau_data/nifti/550/Shank_550c-labels.nii\n",
      "Added to train data: 1 :\t /tmp/broseau_data/nifti/550/Shank_550c.ibw.nii\n",
      "Added to train data: 2 :\t /tmp/broseau_data/nifti/497/Shank_497.ibw.nii\n",
      "Added to label data: 2 :\t /tmp/broseau_data/nifti/497/Shank_497-labels.nii\n",
      "Added to train data: 3 :\t /tmp/broseau_data/nifti/499/Shank_499.ibw.nii\n",
      "Added to label data: 3 :\t /tmp/broseau_data/nifti/499/Shank_499-labels.nii\n",
      "Added to train data: 4 :\t /tmp/broseau_data/nifti/496/Shank_496.ibw.nii\n",
      "Added to label data: 4 :\t /tmp/broseau_data/nifti/496/Shank_496-labels.nii\n",
      "Added to train data: 5 :\t /tmp/broseau_data/nifti/523/Shank_523.ibw.nii\n",
      "Added to label data: 5 :\t /tmp/broseau_data/nifti/523/Shank_523-labels.nii\n",
      "Added to label data: 6 :\t /tmp/broseau_data/nifti/522/Shank_522-labels.nii\n",
      "Added to train data: 6 :\t /tmp/broseau_data/nifti/522/Shank_522.ibw.nii\n",
      "Added to train data: 7 :\t /tmp/broseau_data/nifti/516/Shank_516.ibw.nii\n",
      "Added to label data: 7 :\t /tmp/broseau_data/nifti/516/Shank_516-labels.nii\n",
      "Added to label data: 8 :\t /tmp/broseau_data/nifti/536/Shank_536-labels.nii\n",
      "Added to train data: 8 :\t /tmp/broseau_data/nifti/536/Shank_536.ibw.nii\n",
      "Added to label data: 9 :\t /tmp/broseau_data/nifti/498/Shank_498-labels.nii\n",
      "Added to train data: 9 :\t /tmp/broseau_data/nifti/498/Shank_498.ibw.nii\n",
      "Added to train data: 10 :\t /tmp/broseau_data/nifti/521/Shank_521.ibw.nii\n",
      "Added to label data: 10 :\t /tmp/broseau_data/nifti/521/Shank_521-labels.nii\n",
      "Added to train data: 11 :\t /tmp/broseau_data/nifti/528/Shank_528.ibw.nii\n",
      "Added to label data: 11 :\t /tmp/broseau_data/nifti/528/Shank_528-labels.nii\n",
      "Added to label data: 12 :\t /tmp/broseau_data/nifti/534/Shank_534-labels.nii\n",
      "Added to train data: 12 :\t /tmp/broseau_data/nifti/534/Shank_534.ibw.nii\n",
      "Added to label data: 13 :\t /tmp/broseau_data/nifti/541/Shank_541-labels.nii\n",
      "Added to train data: 13 :\t /tmp/broseau_data/nifti/541/Shank_541.ibw.nii\n",
      "Added to label data: 14 :\t /tmp/broseau_data/nifti/542/Shank_542-labels.nii\n",
      "Added to train data: 14 :\t /tmp/broseau_data/nifti/542/Shank_542.ibw.nii\n",
      "Added to train data: 15 :\t /tmp/broseau_data/nifti/515/Shank_515.ibw.nii\n",
      "Added to label data: 15 :\t /tmp/broseau_data/nifti/515/Shank_515-labels.nii\n",
      "Added to train data: 16 :\t /tmp/broseau_data/nifti/506/Shank_506.ibw.nii\n",
      "Added to label data: 16 :\t /tmp/broseau_data/nifti/506/Shank_506-labels.nii\n",
      "Added to train data: 17 :\t /tmp/broseau_data/nifti/520/Shank_520.ibw.nii\n",
      "Added to label data: 17 :\t /tmp/broseau_data/nifti/520/Shank_520-labels.nii\n",
      "Dataset shape:  (18, 128, 128, 128)\n"
     ]
    }
   ],
   "source": [
    "# Getting all the data files\n",
    "dataset_data, dataset_labels = [], []\n",
    "i_data, i_label = 0, 0\n",
    "for subdir, dirs, files in os.walk(DATA_DIR):\n",
    "    for file in files:\n",
    "        filename = os.path.join(subdir, file)\n",
    "        if filename.endswith(\".nii\") and not \"nifti_em\" in filename:\n",
    "            # Adding source data into dataset_data\n",
    "            if not (\"-labels\" in filename):\n",
    "                dataset_data.append(nib.load(filename).get_fdata())\n",
    "                print(\"Added to train data:\", i_data,\":\\t\", filename)\n",
    "                i_data += 1\n",
    "            # Adding label data into dataset_labels\n",
    "            else:\n",
    "                dataset_labels.append(nib.load(filename).get_fdata())\n",
    "                print(\"Added to label data:\", i_label, \":\\t\", filename)\n",
    "                i_label += 1\n",
    "            \n",
    "dataset_data, dataset_labels = np.array(dataset_data), np.array(dataset_labels)\n",
    "\n",
    "# Verifying training data is valid\n",
    "assert(dataset_data.shape == dataset_labels.shape)\n",
    "print(\"Dataset shape: \", dataset_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def splitTrainingData(data, proportion):\n",
    "    split_index = int(len(data)*proportion)\n",
    "    tmp = np.split(data, [split_index, len(data)])\n",
    "    return tmp[0], tmp[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data: 14 \n",
      "Testing data:  4\n"
     ]
    }
   ],
   "source": [
    "# Splitting data into testing and training sets\n",
    "X_train, X_test  = splitTrainingData(dataset_data, TRAIN_DATA_PROPORTION)\n",
    "Y_train, Y_test  = splitTrainingData(dataset_labels, TRAIN_DATA_PROPORTION)\n",
    "\n",
    "# Verifying training data is valid\n",
    "assert(X_train.shape == Y_train.shape and X_test.shape == Y_test.shape)\n",
    "print(\"Training data:\", X_train.shape[0], \"\\nTesting data: \", X_test.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.0\n",
      "2.0\n",
      "2.0\n",
      "2.0\n",
      "2.0\n",
      "4.0\n",
      "2.0\n",
      "2.0\n",
      "2.0\n",
      "0.0\n",
      "2.0\n",
      "2.0\n",
      "2.0\n",
      "2.0\n",
      "2.0\n",
      "2.0\n",
      "2.0\n",
      "2.0\n"
     ]
    }
   ],
   "source": [
    "# Printing maximum value for each dataset\n",
    "for i in Y_train:\n",
    "    print(np.max(i))\n",
    "for i in Y_test:\n",
    "    print(np.max(i))\n",
    "\n",
    "def removeUnusedLabels(dataset):\n",
    "    f = np.vectorize(lambda x: 1 if x == 2. else 0, otypes=[np.int8])\n",
    "    return f(dataset)\n",
    "\n",
    "Y_train = removeUnusedLabels(Y_train)\n",
    "Y_test  = removeUnusedLabels(Y_test)\n",
    "\n",
    "# Verifying label data validity\n",
    "assert np.max(Y_train) == 1., \"Y_train has \"+str(np.max(Y_train))+\", expected 1.\"\n",
    "assert np.max(Y_test) == 1., \"Y_test has \"+str(np.max(Y_test))+\", expected 1.\"\n",
    "assert(X_train.shape == Y_train.shape and X_test.shape == Y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Augmenting dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating model\n",
    "#498, 516, 523\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#taken from https://github.com/zhixuhao/unet/blob/master/model.py\n",
    "#TODO: verify shapes and network in itself\n",
    "\n",
    "import skimage.io as io\n",
    "import skimage.transform as trans\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import *\n",
    "from tensorflow.keras.layers import *\n",
    "from tensorflow.keras.optimizers import *\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, LearningRateScheduler\n",
    "from tensorflow.keras import backend as keras\n",
    "\n",
    "\n",
    "def unet3D(pretrained_weights = None,input_size = (128,128,128,1)):\n",
    "    inputs = Input(input_size)\n",
    "    conv1 = Conv3D(64, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(inputs)\n",
    "    conv1 = Conv3D(64, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv1)\n",
    "    pool1 = MaxPooling3D(pool_size=(2, 2, 2))(conv1)\n",
    "    conv2 = Conv3D(128, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(pool1)\n",
    "    conv2 = Conv3D(128, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv2)\n",
    "    pool2 = MaxPooling3D(pool_size=(2, 2, 2))(conv2)\n",
    "    conv3 = Conv3D(256, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(pool2)\n",
    "    conv3 = Conv3D(256, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv3)\n",
    "    pool3 = MaxPooling3D(pool_size=(2, 2, 2))(conv3)\n",
    "    conv4 = Conv3D(512, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(pool3)\n",
    "    conv4 = Conv3D(512, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv4)\n",
    "    drop4 = Dropout(0.5)(conv4)\n",
    "    pool4 = MaxPooling3D(pool_size=(2, 2, 2))(drop4)\n",
    "\n",
    "    conv5 = Conv3D(1024, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(pool4)\n",
    "    conv5 = Conv3D(1024, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv5)\n",
    "    drop5 = Dropout(0.5)(conv5)\n",
    "\n",
    "    up6 = Conv3D(512, 2, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(UpSampling3D(size = (2,2,2))(drop5))\n",
    "    merge6 = concatenate([drop4,up6], axis = 4)\n",
    "    conv6 = Conv3D(512, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(merge6)\n",
    "    conv6 = Conv3D(512, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv6)\n",
    "\n",
    "    up7 = Conv3D(256, 2, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(UpSampling3D(size = (2,2,2))(conv6))\n",
    "    merge7 = concatenate([conv3,up7], axis = 4)\n",
    "    conv7 = Conv3D(256, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(merge7)\n",
    "    conv7 = Conv3D(256, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv7)\n",
    "\n",
    "    up8 = Conv3D(128, 2, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(UpSampling3D(size = (2,2,2))(conv7))\n",
    "    merge8 = concatenate([conv2,up8], axis = 4)\n",
    "    conv8 = Conv3D(128, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(merge8)\n",
    "    conv8 = Conv3D(128, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv8)\n",
    "\n",
    "    up9 = Conv3D(64, 2, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(UpSampling3D(size = (2,2,2))(conv8))\n",
    "    merge9 = concatenate([conv1,up9], axis = 4)\n",
    "    conv9 = Conv3D(64, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(merge9)\n",
    "    conv9 = Conv3D(64, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv9)\n",
    "    conv9 = Conv3D(2, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv9)\n",
    "    conv10 = Conv3D(1, 1, activation = 'sigmoid')(conv9)\n",
    "\n",
    "    model = Model(inputs, conv10)\n",
    "\n",
    "    model.compile(optimizer = Adam(lr = 1e-4), loss = 'binary_crossentropy', metrics = ['accuracy'])\n",
    "    \n",
    "    #model.summary()\n",
    "\n",
    "    if(pretrained_weights):\n",
    "        model.load_weights(pretrained_weights)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = unet3D()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python-venv",
   "language": "python",
   "name": "python-venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
